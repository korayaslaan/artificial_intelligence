{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC Archive.ipynb adlı not defterinin kopyası",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqAyfidpdDfpAmAGfeUPYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korayaslaan/artificial_intelligence/blob/main/BBC_Archive_ipynb_adl%C4%B1_not_defterinin_kopyas%C4%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29TcrOD4Fgz9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import urllib3\n",
        "from pandas import DataFrame\n",
        "import csv\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import csv\n",
        "import concurrent\n",
        "import multiprocessing\n",
        "from multiprocessing import pool\n",
        "import io\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dateCreator():\n",
        "  #https://web.archive.org/__wb/calendarcaptures/2?url=https%3A%2F%2Fwww.bbc.co.uk&date=20200105\n",
        "  ser_date = pd.Series(pd.date_range('20010601', periods=15000))\n",
        "  link = \"https://web.archive.org/__wb/calendarcaptures/2?url=https%3A%2F%2Fwww.bbc.co.uk&date=\"\n",
        "  a=3000\n",
        "  b=3050 #8042 son\n",
        "  count=0\n",
        "  count2=0\n",
        "  count3=0\n",
        "  for j in range(a,b):\n",
        "    dateEnd  = ser_date[j].strftime(\"%Y%m%d\")\n",
        "    url = \"{}{}\".format(link,dateEnd)\n",
        "    # print(url)\n",
        "\n",
        "    LinkCreator(url)\n",
        "\n",
        "    count+=1\n",
        "    count2+=1\n",
        "    time.sleep(0.5)\n",
        "    if(count==40):\n",
        "      time.sleep(3)\n",
        "      count = 0\n",
        "    if(count2==150):\n",
        "      time.sleep(5)\n",
        "      count3+=count2\n",
        "      count2 = 0\n",
        "      print(\"Şuanda\"+count3+\"Tarih Kademesi Tarandı\")\n",
        "  #https://web.archive.org/__wb/calendarcaptures/2?url=https%3A%2F%2Fwww.bbc.co.uk&date=2020&groupby=day"
      ],
      "metadata": {
        "id": "EJbQ7zRBJ5Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LinkCreator(link):\n",
        "  response = requests.get(link)\n",
        "  response = str(response.content)\n",
        "  # print(response)\n",
        "  if(len(response) < 10):\n",
        "    return\n",
        "  response = response.split(\"items\")\n",
        "  responseİtems = response[1]\n",
        "  responseİtems = responseİtems.replace(\"[\", \"\")\n",
        "  responseİtems = responseİtems.replace(\"]\", \"\")\n",
        "  responseİtems = responseİtems.replace(\":\", \"\")\n",
        "  responseİtems = responseİtems.replace('\"', \"\")\n",
        "  responseİtems = responseİtems.replace(\"'\", \"\")\n",
        "  responseİtems = responseİtems.split(\",\")\n",
        "  lenResponse = len(responseİtems)\n",
        "  links = []\n",
        "  linkLenght = len(link)\n",
        "  linkDate = link[linkLenght-8:linkLenght]\n",
        "  for i in range(0,lenResponse,3):\n",
        "    resİtem = len(responseİtems[i])\n",
        "    if(resİtem < 7):\n",
        "      if(responseİtems[i+1] == \"200\"):\n",
        "        # print(responseİtems[i]+\" \"+responseİtems[i+1])\n",
        "        links.append(responseİtems[i])\n",
        "      # print(responseİtems[i])\n",
        "  #https://web.archive.org/web/20200105232923/https://www.bbc.co.uk\n",
        "  # print(len(links))\n",
        "  if(len(links)==0):\n",
        "    return\n",
        "  urlHead = \"https://web.archive.org/web/\"\n",
        "  urlFooter = \"/https://www.bbc.co.uk\"\n",
        "  url = \"{}{}{}{}\".format(urlHead,linkDate,links[len(links)-1],urlFooter)\n",
        "  with open(\"UrlNews.txt\", 'a') as file: \n",
        "     file.write(url +'\\n')\n",
        "    #  print(url)"
      ],
      "metadata": {
        "id": "AgfEC9NIXJp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dateCreator()"
      ],
      "metadata": {
        "id": "amb6A-JTFkTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_link(i):\n",
        "     r = requests.get(i)   \n",
        "     soup = BeautifulSoup(r.content, 'html5lib')\n",
        "     Links = []\n",
        "     AllLinksPage = []\n",
        "     for a in soup.find_all('a', href=True):\n",
        "       link = a['href']\n",
        "       result = link.endswith(\".stm\")\n",
        "       if(result == True):\n",
        "         result2 = link.startswith(\"/web\")\n",
        "         if(result2==True):\n",
        "            link = \"https://web.archive.org\" + link\n",
        "            with open(\"NewsLinkAll.txt\", 'a') as file: \n",
        "               file.write(link +'\\n')\n",
        "         else:\n",
        "            with open(\"NewsLinkAll.txt\", 'a') as file: \n",
        "               file.write(link +'\\n')"
      ],
      "metadata": {
        "id": "Lnfx5S2OFkQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LinkCreator(\"https://web.archive.org/__wb/calendarcaptures/2?url=https%3A%2F%2Fwww.bbc.co.uk&date=20200105\")"
      ],
      "metadata": {
        "id": "J9qDUrfxfubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_link(\"https://web.archive.org/web/20000620161051/http://www.bbc.co.uk/\")"
      ],
      "metadata": {
        "id": "JawwWQn-FtGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NewsLink=[]\n",
        "with open(\"UrlNews.txt\",'r',newline='') as f:\n",
        "  for i in f.readlines():    \n",
        "    NewsLink.append(i)\n",
        "\n",
        "t1=time.time()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(get_link,i.strip()) for i in NewsLink]\n",
        "\n",
        "print(time.time()-t1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5EaHpyeFkVV",
        "outputId": "bd5670f4-97b9-42d4-83e0-702e89a14c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.748039484024048\n"
          ]
        }
      ]
    }
  ]
}