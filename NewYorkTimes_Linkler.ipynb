{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewYorkTimes Linkler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBif94itcVJrdteZhJnxYZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korayaslaan/artificial_intelligence/blob/main/NewYorkTimes_Linkler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUY9LTK_7NQ6"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import re\n",
        "import concurrent\n",
        "import multiprocessing\n",
        "from multiprocessing import pool\n",
        "import io\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dateCreator():\n",
        "    science = []\n",
        "    tech = []\n",
        "    #https://www.nytimes.com/search?dropmab=false&endDate=20210104&query=science&sort=best&startDate=20210104 06.01.1981\n",
        "    #https://www.nytimes.com/search?dropmab=false&endDate=20210104&query=tech&sort=best&startDate=20210104    10.01.1982\n",
        "    ser_date1 = pd.Series(pd.date_range('19810106', periods=15000))\n",
        "    ser_date2 = pd.Series(pd.date_range('19820110', periods=15000))\n",
        "    linkscience1 = \"https://www.nytimes.com/search?dropmab=false&endDate=\"\n",
        "    linkscience2 = \"&query=science&sort=best&startDate=\"\n",
        "    linktech1 = \"https://www.nytimes.com/search?dropmab=false&endDate=\"\n",
        "    linktech2 = \"&query=tech&sort=best&startDate=\"\n",
        "    for j in range(10031,10052): #0-14974\n",
        "        dateEnd  = ser_date1[j].strftime(\"%Y%m%d\")\n",
        "        science.append(\"{}{}{}{}\".format(linkscience1,dateEnd,linkscience2,dateEnd))\n",
        "        # print(\"{}{}{}{}\".format(linkscience1,dateEnd,linkscience2,dateEnd))\n",
        "\n",
        "    for j in range(10000,10002):#0-14605 \n",
        "        dateEnd  = ser_date2[j].strftime(\"%Y%m%d\")\n",
        "        tech.append(\"{}{}{}{}\".format(linktech1,dateEnd,linktech2,dateEnd))\n",
        "        # print(\"{}{}{}{}\".format(linktech1,dateEnd,linktech2,dateEnd))\n",
        "\n",
        "    return science,tech"
      ],
      "metadata": {
        "id": "cLrNOK7i7SUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_link(i):\n",
        "    print(i)\n",
        "    driver = webdriver.Chrome (executable_path=\"C:\\\\Users\\\\koray\\\\Desktop\\\\chromedriver.exe\")#masaüstüne \n",
        "    driver.maximize_window()\n",
        "    driver.get(i)\n",
        "    global soup2\n",
        "    sayfa_kaynağı = driver.page_source\n",
        "    soup = BeautifulSoup(sayfa_kaynağı, \"html.parser\")\n",
        "    pagelong = soup.find(\"p\", attrs = {\"class\":\"css-nayoou\"}).getText()\n",
        "    pagelong = pagelong.split(\" \")\n",
        "    pagelong = pagelong[1]\n",
        "    pagelong = int(pagelong)\n",
        "    countNews = int(pagelong)\n",
        "    pagelong = pagelong / 10\n",
        "    pagelong = int(pagelong)\n",
        "    if(countNews%10==0):\n",
        "        pagelong-=1\n",
        "    print(\"Kaç Kere Tıklayacak \" + str(pagelong))\n",
        "    last_height=0\n",
        "    page=0\n",
        "    count = 0\n",
        "    controldriver= \"\"\n",
        "    while count<pagelong:\n",
        "\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "        if new_height == last_height:\n",
        "            try:\n",
        "                driver.find_element_by_xpath(\"//*[@id='site-content']/div/div[2]/div[2]/div/button\").click()\n",
        "            except:\n",
        "                driver.find_element_by_xpath(\"//*[@id='site-content']/div/div[2]/div[3]/div/button\").click()\n",
        "       \n",
        "            time.sleep(0.5)\n",
        "            count+=1\n",
        "        page+=1\n",
        "        last_height = new_height\n",
        "        \n",
        "    sayfa_kaynağı = driver.page_source\n",
        "    soup2 = BeautifulSoup(sayfa_kaynağı, \"html.parser\")\n",
        "    for a in soup2.find_all('a', href=True):\n",
        "        link = a['href']\n",
        "        for a in range(0,countNews):\n",
        "            if(link.endswith(str(a))):\n",
        "                result = link.startswith(\"/\")\n",
        "                if(result == True):\n",
        "                    link = \"https://www.nytimes.com\" + link\n",
        "                    with open(\"Nytimes.txt\", 'a') as file: \n",
        "                        file.write(link + '\\n')\n",
        "                        print(link)\n",
        "    driver.close()\n"
      ],
      "metadata": {
        "id": "-Ioa2XHeKbIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "science,tech= dateCreator()\n",
        "count = 1\n",
        "for i in science:\n",
        "    print(\"Kaçıncı Link?\" + \"-\"+ str(count))\n",
        "    get_link(i)\n",
        "    count+=1\n",
        "for k in tech:\n",
        "    get_link(k)"
      ],
      "metadata": {
        "id": "qPpQDrlK7SPe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}