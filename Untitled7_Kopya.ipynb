{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Untitled7 - Kopya.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korayaslaan/artificial_intelligence/blob/main/Untitled7_Kopya.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preliminary-canon"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np"
      ],
      "id": "preliminary-canon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "golden-sweden"
      },
      "source": [
        "def datecreator():\n",
        "    t1=datetime(2010,6,1)#en son 2007 04 01 e kadar çalışıyo\n",
        "    t2=datetime(2010,6,6)\n",
        "    date_list=[]\n",
        "    t = timedelta(days = 1)\n",
        "    txt=\"\"\n",
        "    newdate=\"\"\n",
        "    url_in=\"\"\n",
        "   #array_page_link=[]\n",
        "    url=\"https://www.haberturk.com/arama/haberler?tr=Haberler&bitis_tarihi=\"\n",
        "    dates = np.arange(t1, t2, t).astype(datetime) #linklerin çekileceği sayfanın linkini oluştuyor \n",
        "    for j in dates:\n",
        "      # print(i.strftime('%d-%m-%Y'))\n",
        "        newdate=j.strftime('%Y-%m-%d')\n",
        "        url_in=\"{}{}{}{}\".format(url,newdate,\"&baslangic_tarihi=\",newdate)\n",
        "        #print(url_in)\n",
        "        date_list.append(url_in)\n",
        "    #https://www.haberturk.com/arama/haberler?tr=Haberler&baslangic_tarihi=2021-07-15\n",
        "    #https://www.haberturk.com/arama/haberler?tr=Haberler&baslangic_tarihi=2021-07-02\n",
        "    #https://www.haberturk.com/arama/Haberler?tr=Haberler&bitis_tarihi=2021-07-01&baslangic_tarihi=2021-07-01\n",
        "    return date_list"
      ],
      "id": "golden-sweden",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mexican-porter"
      },
      "source": [
        "def create_scroll_long():\n",
        "    dates = datecreator()\n",
        "    scroll = []\n",
        "    for i in dates:\n",
        "        r = requests.get(i)   \n",
        "        soup = BeautifulSoup(r.content, 'html5lib')\n",
        "        link2 = soup.find(\"div\" , attrs={\"class\" : \"htSearchResultInfo\"}).span.getText()\n",
        "        scroll_number = int(link2) // 20   #her bir scrolde ortalama 20-30 arası çekiyo ama başlangıç ve son scrollee beraber 20 iyi\n",
        "        scroll.append(scroll_number)\n",
        "    return scroll"
      ],
      "id": "mexican-porter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fluid-reynolds"
      },
      "source": [
        "def veri_cek(i,j):\n",
        "    sayfa =i\n",
        "    driver_path = \"C:/Users/koray/Desktop/chromedriver.exe\"\n",
        "    browser = webdriver.Chrome(driver_path)\n",
        "    browser.get(j)\n",
        "    a = 0\n",
        "    dizi = []\n",
        "    dizi2 = []\n",
        "    result = True\n",
        "    while a < sayfa:\n",
        "       \n",
        "        lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
        "        i=0\n",
        "        while i<1:\n",
        "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(2)\n",
        "            newHeight = browser.execute_script(\"return document.body.scrollHeight\") #scroll ile aşağı inme işlemi \n",
        "\n",
        "            if newHeight == lastHeight:\n",
        "                break\n",
        "            else:\n",
        "                lastHeight = newHeight\n",
        "            i = i+1\n",
        "            a+=1\n",
        "           \n",
        "    sayfa_kaynağı = browser.page_source\n",
        "    soup = BeautifulSoup(sayfa_kaynağı, \"html.parser\")\n",
        "    test = soup.find_all('a', attrs={\"target\" : \"_blank\"})\n",
        "    \n",
        "    for i in test:\n",
        "        result = i['href'].startswith(('/arama','https:','//','/spor'), 0, 7) #standartlara uygun linkleri çekiyor \n",
        "        link = i['href']\n",
        "        if result == False:\n",
        "            if len(link)>35:\n",
        "                date = \"{}{}\".format('https://www.haberturk.com',link)\n",
        "                dizi.append(date)\n",
        "                #ilk sayfa 40 haber çekiyo\n",
        "    return dizi"
      ],
      "id": "fluid-reynolds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "democratic-version"
      },
      "source": [
        "def get_link():\n",
        "    scroll_piece = create_scroll_long()\n",
        "    link = datecreator()\n",
        "    dizi = []\n",
        "    dizi2 = []\n",
        "    count = 0\n",
        "    for i in link:\n",
        "        j = scroll_piece[count]     #scroll sayısını ve sayfa linkini gönderiyor \n",
        "        try:\n",
        "            dizi = veri_cek(j,i)\n",
        "            dizi2.append(dizi)\n",
        "            count+=1\n",
        "        except:\n",
        "            print(\"Başarısız\")\n",
        "   \n",
        "    return dizi2"
      ],
      "id": "democratic-version",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apart-appeal"
      },
      "source": [
        "def get_url():\n",
        "    dizi = get_link()\n",
        "    dizi2 = []\n",
        "    for i in dizi:\n",
        "        for j in i:\n",
        "            if not j in dizi2:           #çekilen linklerde aynı olan verileri teke düşürüyor \n",
        "                dizi2.append(j)\n",
        "    return dizi2"
      ],
      "id": "apart-appeal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appointed-professional"
      },
      "source": [
        "def get_data_from_url(url):\n",
        "    result = []\n",
        "    r = requests.get(url)            #haber metinini çekiyor \n",
        "    soup = BeautifulSoup(r.content, 'html5lib')\n",
        "    contect = soup.find(\"article\", attrs = {\"class\":\"content\"})\n",
        "    date = soup.find(\"span\", attrs = {\"class\":\"date\"}).getText()\n",
        "    date = date.split()\n",
        "    date = date[0]\n",
        "    test = \"\"\n",
        "    for a in contect.find_all('p'):\n",
        "        test= test + a.getText()\n",
        "        \n",
        "        #print(a.getText())\n",
        "    veri = \"{};{};{}\".format(url,date,test)\n",
        "    dosya=\"C:/Users/koray/Desktop/haberturk.txt\"\n",
        "    with open(dosya, 'a') as f:\n",
        "        f.write(veri)\n",
        "        f.write('\\n')\n",
        "        f.close()\n",
        "    result.append(veri)\n",
        "    return result    "
      ],
      "id": "appointed-professional",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "union-diamond",
        "outputId": "1f8e5011-1fc9-43b7-e763-c102cd25ac1f"
      },
      "source": [
        "link = get_url()\n",
        "dizi = []\n",
        "for i in link:\n",
        "    try:\n",
        "        get_data_from_url(i)\n",
        "    except:\n",
        "        print(\"*****\")\n",
        "       \n"
      ],
      "id": "union-diamond",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n",
            "*****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reasonable-technician"
      },
      "source": [
        ""
      ],
      "id": "reasonable-technician",
      "execution_count": null,
      "outputs": []
    }
  ]
}