{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Folha News.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOgaiWN0peq8JEeOnzecVy6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korayaslaan/artificial_intelligence/blob/main/Folha_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tBgmGJ1e77gH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "import re\n",
        "import urllib3\n",
        "from pandas import DataFrame\n",
        "import csv\n",
        "import datetime\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import csv\n",
        "import concurrent\n",
        "import multiprocessing\n",
        "from multiprocessing import pool\n",
        "import io\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dateCreator():\n",
        "  links = []  \n",
        "  #https://search.folha.uol.com.br/search?q=science&periodo=personalizado&sd=01%2F01%2F2021&ed=01%2F02%2F2021&site=todos\n",
        "  #https://search.folha.uol.com.br/search?q=tech&periodo=personalizado&sd=01%2F01%2F2021&ed=09%2F01%2F2022&site=todos\n",
        "  ser_date = pd.Series(pd.date_range('20010101', periods=8400))\n",
        "  link1 = \"https://search.folha.uol.com.br/search?q=\"\n",
        "  link2 = \"&periodo=personalizado&sd=\"\n",
        "  link3 = \"&ed=\"\n",
        "  link4 = \"&site=todos\"\n",
        "  category  = ['science','tech']\n",
        "  a=6000\n",
        "  b=6500  #7315 son\n",
        "  for i in category:\n",
        "    for j in range(a,b):\n",
        "      dateEnd  = ser_date[j].strftime(\"%d.%m.%Y\")\n",
        "      dateEnd = dateEnd.split(\".\")\n",
        "      dateEnd = dateEnd[0] + \"%2F\" + dateEnd[1] + \"%2F\" +dateEnd[2]\n",
        "      links.append(\"{}{}{}{}{}{}{}\".format(link1,i,link2,dateEnd,link3,dateEnd,link4))\n",
        "  return links"
      ],
      "metadata": {
        "id": "ns-R0g_j8CjS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_link(i):\n",
        "     r = requests.get(i)   \n",
        "     soup = BeautifulSoup(r.content, 'html5lib')\n",
        "     for a in soup.find_all('a', href=True):\n",
        "       link = a['href']\n",
        "       result = link.endswith(\".shtml\")\n",
        "       if(result == True):\n",
        "         if(len(link)>80):\n",
        "           with open(\"NewsLinks.txt\", 'a') as file: \n",
        "             file.write(link + '\\n')"
      ],
      "metadata": {
        "id": "IN5s76a78Cfh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = dateCreator()\n",
        "with concurrent.futures.ProcessPoolExecutor() as execut:\n",
        "  b_res=[execut.submit(get_link,i.strip()) for i in links]"
      ],
      "metadata": {
        "id": "upiAQsJoChZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get_link(\"https://search.folha.uol.com.br/search?q=science&periodo=personalizado&sd=01%2F02%2F2015&ed=09%2F03%2F2015&site=todos\")"
      ],
      "metadata": {
        "id": "SBMIX_GD8Cde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def creator(url):\n",
        "#     r = requests.get(url)   \n",
        "#     soup = BeautifulSoup(r.content, 'html5lib')\n",
        "#     array = soup.find(\"div\", attrs = {\"id\":\"articleNew\"})\n",
        "#     content_array = \"\"\n",
        "#     for a in array.find_all('p'):\n",
        "#       content_array = content_array + a.getText()\n",
        "    \n",
        "#     print(content_array)\n",
        "#     title = array.find(\"h1\").getText()\n",
        "#     # category = soup.find(\"h4\", attrs = {\"class\":\"artikel\"}).getText()\n",
        "#     date = soup.find(\"p\", attrs = {\"class\":\"publish\"}).getText()\n",
        "#     # date = re.sub(\"^\\s+|\\s+$\", \"\", date, flags=re.UNICODE)\n",
        "#     # date = date.split(\"\\n\")\n",
        "#     # date = date[1]\n",
        "#     # content_string = \"\"\n",
        "#     # for m in content_array:\n",
        "#     #    content_string = content_string+\" \"+m\n",
        "#     # w_data=\"{};{};{};{};{}\".format(url,date,category,title,content_string)\n",
        "#     # with open(\"contentAll.txt\", 'a') as file: \n",
        "#     #   file.write(w_data+'\\n')"
      ],
      "metadata": {
        "id": "9otfuvqw_nBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creator(\"https://www1.folha.uol.com.br/internacional/en/scienceandhealth/2015/09/1679373-light-aircraft-made-by-brazilian-students-beats-world-records.shtml\")"
      ],
      "metadata": {
        "id": "iz_Mdun0_m-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}